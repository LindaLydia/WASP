llm models: ['gpt2-xl', 'llama-2-7b-chat-hf', 'vicuna-7b-1.5v', 'opt-6.7b', 'chatglm3-6b-base', 'flan-t5-xl'], inner samples: [1000, 1000, 1000, 1000, 1000, 1000]
BETA change with #data, BETA=4.408560690471575
learning rate: 2e-05
seed: 0
load tokenizer for bert first
4 steps are taken to construct the total dataset
[INFO] extend 1200 samples in total for each step
Finish copying original 200 samples, with target of copying 200 samples
Finish copying original 200 samples, with target of copying 200 samples
Finish copying original 200 samples, with target of copying 200 samples
Finish copying original 200 samples, with target of copying 200 samples
Finish copying original 200 samples, with target of copying 200 samples
Finish copying original 200 samples, with target of copying 200 samples
Finish copying original 200 samples, with target of copying 200 samples
Finish copying original 200 samples, with target of copying 200 samples
Finish copying original 200 samples, with target of copying 200 samples
Finish copying original 200 samples, with target of copying 200 samples
Finish copying original 200 samples, with target of copying 200 samples
Finish copying original 200 samples, with target of copying 200 samples
num of use syn samples in total: 6000
num of current syn samples for step 0: tensor([200, 200, 200, 200, 200, 200], device='cuda:0')
train_dataset for gpt2-xl
[debug] sample_text has length 180
train_dataset for llama-2-7b-chat-hf
[debug] sample_text has length 180
train_dataset for vicuna-7b-1.5v
[debug] sample_text has length 180
train_dataset for opt-6.7b
[debug] sample_text has length 180
train_dataset for chatglm3-6b-base
[debug] sample_text has length 180
train_dataset for flan-t5-xl
[debug] sample_text has length 180
golden train data
sample_per_class=[76, 24]
test dataset
sample_per_class=[0, -1]
